url: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-significantterms-aggregation.html
doc: |
    An aggregation that returns interesting or unusual occurrences of terms in a set.

    Example use cases:

        - Suggesting "H5N1" when users search for "bird flu" in text
        - Identifying the merchant that is the "common point of compromise" from the
        transaction history of credit card owners reporting loss
        - Suggesting keywords relating to stock symbol $ATI for an automated news classifier
        - Spotting the fraudulent doctor who is diagnosing more than their fair share of whiplash injuries
        - Spotting the tire manufacturer who has a disproportionate number of blow-outs

    In all these cases the terms being selected are not simply the most popular terms in a set.
    They are the terms that have undergone a significant change in popularity measured between
    a foreground and background set. If the term "H5N1" only exists in 5 documents in a 10
    million document index and yet is found in 4 of the 100 documents that make up a user’s
    search results that is significant and probably very relevant to their search.
    `5/10,000,000` vs `4/100` is a big swing in frequency.

    Warning:

        Picking a free-text field as the subject of a significant terms analysis can be
        expensive! It will attempt to load every unique word into RAM. It is recommended
        to only use this on smaller indices.

# TODO: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-significantterms-aggregation.html#significantterms-aggregation-parameters
#   how to map those different significance algorithm parameters?
parameters:
    field:
        type: str
        required: True
    size:
        type: int
        default: 10
        doc: |
            The size parameter can be set to define how many term buckets should be returned
            out of the overall terms list. By default, the node coordinating the search process
            will request each shard to provide its own top size term buckets and once all shards
            respond, it will reduce the results to the final list that will then be returned to
            the client. This means that if the number of unique terms is greater than size, the
            returned list is slightly off and not accurate (it could be that the term counts are
            slightly off and it could even be that a term that should have been in the top size
            buckets was not returned).
    shard_size:
        type: int
        doc: |
            The higher the requested size is, the more accurate the results will be, but also,
            the more expensive it will be to compute the final results (both due to bigger
            priority queues that are managed on a shard level and due to bigger data transfers
            between the nodes and the client).
            
            The shard_size parameter can be used to minimize the extra work that comes with bigger
            requested size. When defined, it will determine how many terms the coordinating node
            will request from each shard. Once all the shards responded, the coordinating node will
            then reduce them to a final result which will be based on the size parameter - this way,
            one can increase the accuracy of the returned terms and avoid the overhead of streaming
            a big list of buckets back to the client.
    min_doc_count:
        type: int
        default: 1
        doc: |
            It is possible to only return terms that match more than a configured number of hits
            using the min_doc_count option. Default value is 1.
            
            Terms are collected and ordered on a shard level and merged with the terms collected
            from other shards in a second step. However, the shard does not have the information
            about the global document count available. The decision if a term is added to a candidate
            list depends only on the order computed on the shard using local shard frequencies.
            The min_doc_count criterion is only applied after merging local terms statistics of all shards.
            In a way the decision to add the term as a candidate is made without being very certain about
            if the term will actually reach the required min_doc_count. This might cause many (globally)
            high frequent terms to be missing in the final result if low frequent terms populated the
            candidate lists. To avoid this, the shard_size parameter can be increased to allow more
            candidate terms on the shards. However, this increases memory consumption and network traffic.
    shard_min_doc_count:
        type: int
        doc: |
            The parameter shard_min_doc_count regulates the certainty a shard has if the term should
            actually be added to the candidate list or not with respect to the min_doc_count.
            Terms will only be considered if their local shard frequency within the set is higher
            than the shard_min_doc_count. If your dictionary contains many low frequent terms and
            you are not interested in those (for example misspellings), then you can set the
            shard_min_doc_count parameter to filter out candidate terms on a shard level that will
            with a reasonable certainty not reach the required min_doc_count even after merging the
            local counts. shard_min_doc_count is set to 0 per default and has no effect unless
            you explicitly set it.
            
            Note: Setting min_doc_count=0 will also return buckets for terms that didn’t match
            any hit. However, some of the returned terms which have a document count of zero might
            only belong to deleted documents or documents from other types, so there is no warranty
            that a match_all query would find a positive document count for those terms.
            
            Warning: When NOT sorting on doc_count descending, high values of min_doc_count may
            return a number of buckets which is less than size because not enough data was
            gathered from the shards. Missing buckets can be back by increasing shard_size.
            Setting shard_min_doc_count too high will cause terms to be filtered out on a
            shard level. This value should be set much lower than min_doc_count/#shards.
    execution_hint:
        type: str
        default: global_ordinals
        doc: |
            There are different mechanisms by which terms aggregations can be executed:

                - by using field values directly in order to aggregate data per-bucket (`map`)
                - by using global ordinals of the field and allocating one bucket per global ordinal (`global_ordinals`)

            Elasticsearch tries to have sensible defaults so this is something that generally doesn’t need to be configured.

            `global_ordinals` is the default option for keyword field, it uses global ordinals to allocates
            buckets dynamically so memory usage is linear to the number of values of the documents
            that are part of the aggregation scope.

            `map` should only be considered when very few documents match a query. Otherwise the
            ordinals-based execution mode is significantly faster. By default, `map` is only used when
            running an aggregation on scripts, since they don’t have ordinals.
    include:
        type: Union[str, Sequence[str], Mapping[str, int]]
        doc: |
            A [regexp](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html)
            pattern that filters the documents which will be aggregated.

            Alternatively can be a list of strings.

            [Parition expressions](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_filtering_values_with_partitions)
            are also possible.
    exclude:
        type: Union[str, Sequence[str]]
        doc: |
            A [regexp](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html)
            pattern that filters the documents which will be aggregated.

            Alternatively can be a list of strings.
    script:
        type: dict
        doc: |
            Generating the terms using a script
